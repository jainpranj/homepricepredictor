{
    "nbformat_minor": 1, 
    "cells": [
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Requirement already satisfied (use --upgrade to upgrade): numpy in /usr/local/src/conda3_runtime.v11/4.1.1/lib/python3.5/site-packages\nRequirement already satisfied (use --upgrade to upgrade): pandas in /usr/local/src/conda3_runtime.v11/4.1.1/lib/python3.5/site-packages\nCollecting scip\n\u001b[31m  Could not find a version that satisfies the requirement scip (from versions: )\u001b[0m\n\u001b[31mNo matching distribution found for scip\u001b[0m\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 1, 
            "source": "!pip install numpy pandas scip sklearn\n"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 16, 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 17, 
            "source": "# Remove the fields from the data set that we don't want to include in our model\ndel df['house_number']\ndel df['unit_number']\ndel df['street_name']\ndel df['zip_code']\n\n# Replace categorical data with one-hot encoded data\nfeatures_df = pd.get_dummies(df, columns=['garage_type', 'city'])\n\n# Remove the sale price from the feature data\ndel features_df['sale_price']\n\n# Create the X and y arrays\nX = features_df.as_matrix()\ny = df['sale_price'].as_matrix()"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 18, 
            "source": "# Split the data set in a training set (70%) and a test set (30%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "['trained_house_classifier_model.pkl']"
                    }, 
                    "execution_count": 19
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 19, 
            "source": "# Fit regression model\nmodel = ensemble.GradientBoostingRegressor(\n    n_estimators=1000,\n    learning_rate=0.1,\n    max_depth=6,\n    min_samples_leaf=9,\n    max_features=0.1,\n    loss='huber',\n    random_state=0\n)\nmodel.fit(X_train, y_train)\n\n# Save the trained model to a file so we can use it in other programs\njoblib.dump(model, 'trained_house_classifier_model.pkl')"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Training Set Mean Absolute Error: 47976.0114\nTest Set Mean Absolute Error: 59004.6405\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 20, 
            "source": "# Find the error rate on the training set\nmse = mean_absolute_error(y_train, model.predict(X_train))\nprint(\"Training Set Mean Absolute Error: %.4f\" % mse)\n\n# Find the error rate on the test set\nmse = mean_absolute_error(y_test, model.predict(X_test))\nprint(\"Test Set Mean Absolute Error: %.4f\" % mse)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": null, 
            "source": "#Parameters we want to try\nparam_grid = {\n    'n_estimators': [500, 1000, 3000],\n    'max_depth': [4, 6],\n    'min_samples_leaf': [3, 5, 9, 17],\n    'learning_rate': [0.1, 0.05, 0.02, 0.01],\n    'max_features': [1.0, 0.3, 0.1],\n    'loss': ['ls', 'lad', 'huber']\n}\n\n# Define the grid search we want to run. Run it with four cpus in parallel.\ngs_cv = GridSearchCV(model, param_grid, n_jobs=4)\n\n# Run the grid search - on only the training data!\ngs_cv.fit(X_train, y_train)\n\n# Print the parameters that gave us the best result!\nprint(gs_cv.best_params_)\n\n# After running a .....long..... time, the output will be something like\n# {'loss': 'huber', 'learning_rate': 0.1, 'min_samples_leaf': 9, 'n_estimators': 3000, 'max_features': 0.1, 'max_depth': 6}\n\n# That is the combination that worked best.\n\n# Find the error rate on the training set using the best parameters\nmse = mean_absolute_error(y_train, gs_cv.predict(X_train))\nprint(\"Training Set Mean Absolute Error: %.4f\" % mse)\n\n# Find the error rate on the test set using the best parameters\nmse = mean_absolute_error(y_test, gs_cv.predict(X_test))\nprint(\"Test Set Mean Absolute Error: %.4f\" % mse)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "source": "import numpy as np\nfrom sklearn.externals import joblib\n\n# These are the feature labels from our data set\nfeature_labels = np.array(['year_built', 'stories', 'num_bedrooms', 'full_bathrooms', 'half_bathrooms', 'livable_sqft', 'total_sqft', 'garage_sqft', 'carport_sqft', 'has_fireplace', 'has_pool', 'has_central_heating', 'has_central_cooling', 'garage_type_attached', 'garage_type_detached', 'garage_type_none', 'city_Amystad', 'city_Brownport', 'city_Chadstad', 'city_Clarkberg', 'city_Coletown', 'city_Davidfort', 'city_Davidtown', 'city_East Amychester', 'city_East Janiceville', 'city_East Justin', 'city_East Lucas', 'city_Fosterberg', 'city_Hallfort', 'city_Jeffreyhaven', 'city_Jenniferberg', 'city_Joshuafurt', 'city_Julieberg', 'city_Justinport', 'city_Lake Carolyn', 'city_Lake Christinaport', 'city_Lake Dariusborough', 'city_Lake Jack', 'city_Lake Jennifer', 'city_Leahview', 'city_Lewishaven', 'city_Martinezfort', 'city_Morrisport', 'city_New Michele', 'city_New Robinton', 'city_North Erinville', 'city_Port Adamtown', 'city_Port Andrealand', 'city_Port Daniel', 'city_Port Jonathanborough', 'city_Richardport', 'city_Rickytown', 'city_Scottberg', 'city_South Anthony', 'city_South Stevenfurt', 'city_Toddshire', 'city_Wendybury', 'city_West Ann', 'city_West Brittanyview', 'city_West Gerald', 'city_West Gregoryview', 'city_West Lydia', 'city_West Terrence'])\n\n# Load the trained model created with train_model.py\nmodel = joblib.load('trained_house_classifier_model.pkl')\n\n# Create a numpy array based on the model's feature importances\nimportance = model.feature_importances_\n\n# Sort the feature labels based on the feature importance rankings from the model\nfeauture_indexes_by_importance = importance.argsort()\n\n# Print each feature label, from most important to least important (reverse order)\nfor index in feauture_indexes_by_importance:\n    print(\"{} - {:.2f}%\".format(feature_labels[index], (importance[index] * 100.0)))"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "source": "from sklearn.externals import joblib\n\n# Load the model we trained previously\nmodel = joblib.load('trained_house_classifier_model.pkl')\n\n# For the house we want to value, we need to provide the features in the exact same\n# arrangement as our training data set.\nhouse_to_value = [\n    # House features\n    2006,   # year_built\n    1,      # stories\n    4,      # num_bedrooms\n    3,      # full_bathrooms\n    0,      # half_bathrooms \n    2200,   # livable_sqft\n    2350,   # total_sqft\n    0,      # garage_sqft\n    0,      # carport_sqft\n    True,   # has_fireplace\n    False,  # has_pool\n    True,   # has_central_heating\n    True,   # has_central_cooling\n    \n    # Garage type: Choose only one\n    0,      # attached\n    0,      # detached\n    1,      # none\n    \n    # City: Choose only one\n    0,      # Amystad\n    1,      # Brownport\n    0,      # Chadstad\n    0,      # Clarkberg\n    0,      # Coletown\n    0,      # Davidfort\n    0,      # Davidtown\n    0,      # East Amychester\n    0,      # East Janiceville\n    0,      # East Justin\n    0,      # East Lucas\n    0,      # Fosterberg\n    0,      # Hallfort\n    0,      # Jeffreyhaven\n    0,      # Jenniferberg\n    0,      # Joshuafurt\n    0,      # Julieberg\n    0,      # Justinport\n    0,      # Lake Carolyn\n    0,      # Lake Christinaport\n    0,      # Lake Dariusborough\n    0,      # Lake Jack\n    0,      # Lake Jennifer\n    0,      # Leahview\n    0,      # Lewishaven\n    0,      # Martinezfort\n    0,      # Morrisport\n    0,      # New Michele\n    0,      # New Robinton\n    0,      # North Erinville\n    0,      # Port Adamtown\n    0,      # Port Andrealand\n    0,      # Port Daniel\n    0,      # Port Jonathanborough\n    0,      # Richardport\n    0,      # Rickytown\n    0,      # Scottberg\n    0,      # South Anthony\n    0,      # South Stevenfurt\n    0,      # Toddshire\n    0,      # Wendybury\n    0,      # West Ann\n    0,      # West Brittanyview\n    0,      # West Gerald\n    0,      # West Gregoryview\n    0,      # West Lydia\n    0       # West Terrence\n]\n\n# scikit-learn assumes you want to predict the values for lots of houses at once, so it expects an array.\n# We just want to look at a single house, so it will be the only item in our array.\nhomes_to_value = [\n    house_to_value\n]\n\n# Run the model and make a prediction for each house in the homes_to_value array\npredicted_home_values = model.predict(homes_to_value)\n\n# Since we are only predicting the price of one house, just look at the first prediction returned\npredicted_value = predicted_home_values[0]\n\nprint(\"This house has an estimated value of ${:,.2f}\".format(predicted_value))\n\n"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 1.6", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "name": "python", 
            "file_extension": ".py", 
            "version": "3.5.2", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "pygments_lexer": "ipython3"
        }
    }, 
    "nbformat": 4
}